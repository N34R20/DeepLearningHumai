# Posibles preguntas para el parcial de NLP

## Clase 1 Introduccion

1. Tokenizacion

2. Distancia de Edicion

3. Busqueda difusa de strings

4. Representacion

5. TF-IDF

6. Similitud coseno

7. N gramas



## Clase 2 Redes Neuronales Recurrentes

1. modelos markovianos

2. redes recurrentes

3. problemas con las predicciones

4. teacher forcing

5. muestreo de secuencias

6. perplejidad

7. backpropagation en el tiempo

8. gradient clipping

9. preprocesamiento
 - descripcion del pipeline:
    1. Carga de los datos
    1. Separación de los datos en lotes
    1. Inicialización de parámetros
    1. Definición del modelo
    1. Definición de la función de pérdida
    1. Definición del algoritmo de optimización

10. Long Short-Term Memory (LSTM)    
intenta emular la memoria RAM de la PC


11. Gated Recurrent Units (GRU)

12. Redes bidireccionales y profundas

## Clase 3 Embeddings

1. los vectores one-hot son una mala eleccion:
    - ineficiencia
    - ortogonalidad

2. como codificamos el resultado?

3. semantica distribucional

4. Word2Vec: entrenamiento autosupervisado

5. Skip-gram

6. Continuous Bag of Words (CBOW)

7. Muestreo negativo

8. Entrenamiento

9. GloVe: Embeddings con vectores globales

10. razon de probabilidades de co-ocurrencia

11. funcion de perdida

12. fastText

13. Embeddings pre entrenados

## Clase 4 Seq2Seq

1. descripcion de modelos de secuencia a secuencia

2. arquitectura encoder decoder

3. teacher forcing

4. funcion de perdida


## Clase 5 Atencion

1. traduccion humana

2. diccionarios en python

3. funciones de hash

4. Atencion

5. funciones de atencion:

    1. aditiva
    2. producto escalar
    3. producto escalar generalizado

6. BLEU    

7. Beam search

## Clase 6 Transformers

1. estructura general

2. flujo de tensores

3. auto-atencion

4. codificacion personal

5. conexiones residuales

6. Etapa de Decodificacion

7. capa densa de prediccion

## Clase 7 BERT