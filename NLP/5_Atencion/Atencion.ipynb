{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lSpDS2yrgNot",
        "5xI3YSar4Cpa",
        "Gxp7Rrg44KeW",
        "QhHJzPJVEuO0",
        "TGnuhRJHJTyJ",
        "auoXxGo1sWjt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/5_Atencion/Atencion.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "9ifb0z7xkJrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traducción humana\n",
        "\n",
        "Olvidemos por un momento lo que hemos visto hasta aquí de redes recurrentes\n",
        "\n",
        "Pensemos en un segundo como es que uno traduce normalmente. Como ejemplo de un idioma extraño, tomaremos el japonés.\n",
        "\n",
        "##あの人はミカンを食べた。\n",
        ">_aru hito wa mikan o tabeta_\n",
        "\n",
        "En esta breve oración vemos todos los tipos de caracteres que se usan en el japonés. A saber:\n",
        "\n",
        "* hiragana: あ, の, は, を, べ, た\n",
        "* katakana:ミ, カ, ン\n",
        "* kanji: 人, 食\n",
        "* puntuación: 。\n",
        "\n",
        "Normalmente, son los los caracteres hiragana y katakana los primeros en ser aprendidos y corresponden a silabas. Esto carecen de valor semántico y solo son silabas. El mayor problema suelen ser lo kanji. Hay alrdedor de 2000 kanji de uso diario más otros 2000 en topónimos, términos específicos o nombres técnicos. Dada lo complejidad de estos carateres, existen *diccionarios*, no solo para los extranjeros, sino también para los locales\n",
        "\n",
        "El lector interesado puede usar [jisho.org](http://www.jisho.org) para buscar esos caracteres. No es nuestro interes enseñar a usar esa página, sino usarla para obtener información. En jisho.org, se nos informa lo siguiente:\n",
        "\n",
        "* 人: _persona, alguien_\n",
        "* 食: _comida, comestible_\n",
        "\n",
        "Solo buscando caracteres y algo más ya sabemos que hay algo de una persona y una comida.\n",
        "\n",
        "Si bien hemos terminado con la parte más pesada (busqueda de kanji) nos queda entender que hacen el resto de las partes de nuestra oración.\n",
        "\n",
        "A diferencia del español o el inglés, la noción de palabra en el japonés o el chino carecen de sentido. La razón para señalar esto es algo que el lector notará rápidamente: no hay espacios entre los caracteres. Por lo tanto debemos separar los símbolos en conjuntos de caracteres con contenido gramatical.\n",
        "\n",
        "> Nota: ¡Esto es literamente lo que hacemos cuando tokenizamos! Pero debemos consistentes con la primera oración de esta sección: ***Olvidemos por un momento lo que hemos visto hasta aquí de redes recurrentes***. Es decir, estamos suponiendo que ni siquiera sabemos que es tokenizar.\n",
        "\n",
        "Pues bien, pasemos ahora a separar estas partes.\n",
        "\n",
        "##あの,$~~$人,$~~$は,$~~$ミカン,$~~$を,$~~$食べた,$~~$。\n",
        "\n",
        "Usaremos ahora nuestro **diccionario**, jisho.org para ver que nos dice de esto:\n",
        "\n",
        "* ある → 彼の: aquel\n",
        "* 人: persona\n",
        "* は: indica el tema de la oración\n",
        "* ミカン → 蜜柑 : mandarina\n",
        "* を: indica objeto directo\n",
        "* 食べた → 食べる: comer\n",
        "* 。: punto final (jisho.org parece no tener este carater)\n",
        "\n",
        "Hemos usado \"→\" para indicar cual es la entrada del ***diccionario*** para ese caso. En función de lo anterior podemos pensar que nuestra oración dice algo como lo siguiente:\n",
        "\n",
        "##(aquel) (persona) (**tema**) (mandarina) (**objeto directo**) (comer) (punto final)\n",
        "\n",
        "por lo tanto no debemos esforzarnos mucho más para llegar a la conclusión de que esa oración debería traducirse como:\n",
        "\n",
        "## Aquella persona comió una mandarina.\n",
        "\n",
        "Para contruir esa oración hemos usado mucha información adicional propia del español. Además hay información de la gramática japonesa que hemos omitido:\n",
        "\n",
        "* En español, por lo general, el objeto directo va a continuación de un verbo. por eso no necesitamos marcarlo, a diferencia del japones.\n",
        "* Es español, la mayoría de las veces el tema de la oración está al principio de la misma. Compare:\n",
        "  * Aquella persona comió una mandarina\n",
        "  * Una mandarina es lo que comió aquella perona\n",
        "* En japones casi todas las terminaciones -た indican pasado, por eso pusimos \"comió\".\n",
        "\n",
        "Vemos que la noción de un ***DICCIONARIO*** es una idea muy útil y podría ser de gran ayuda para traducir texto. Por esta razón, uno quisiera tener uno. Curiosamente, `python` tiene una estructura de datos llamada `dictionary`. Sería interesante poder ver si esta estructura nos puede ayudar."
      ],
      "metadata": {
        "id": "lSpDS2yrgNot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diccionarios en `python`"
      ],
      "metadata": {
        "id": "5xI3YSar4Cpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si usted sabe algo de estructura de datos, solo diremos que los diccionarios de python son tablas de hashes.\n",
        "\n",
        "Si usted tiene alguna información sobre hardware y electrónica o arquitectura de las computadoras, solo diremos que un diccionario de python tiene un comportamiento similar a las memorias asociativas usadas en el cache de una PC.\n",
        "\n",
        "Si usted no cae en ninguna de esas dos categorías. Haremos una muy pequeña introducción a como es que un diccionario en `python` busca los diferentes datos almacenados en el.\n",
        "\n",
        "Como dijimos más arriba, un diccionario de Python es una tabla de hashes. Esto quiere decir que se intenta que tanto la búsqueda, como la inserción o la eliminación de un dato ocurra en el menor tiempo posible. Formalmente se busca que los algortimos no dependan de la cantidad de elementos en almacenados en el diccionario. Para hacer esto, se usa una función llamanda \"función hash\".\n",
        "\n",
        "### Funciones de hash\n",
        "\n",
        "La idea de una función de hash, es crear una función que para cualquier dato devuelva un valor único o al menos un valor que sea muy poco probable volver generar. Es decir:\n",
        "\n",
        "Si al string \"perro\" le corresponde el hash 3262323, deber ser imposible o almenos muy poco probable que haya otro string que genere 3262323. De igual modo, dado 3262323 deberíamos ser capaces de obtener \"perro\". Pero la forma de hacerlo debería ser lo suficientemente complicada como para que no tenga sentido hacerlo.\n",
        "\n",
        "El nombre \"hash\" proviene de los llamados \"hash brown\" una suerte de tortilla de papa que se prepara sin huevo. En principio, uno puede desarmar el hash brown para obtener la papa original, pero el tiempo que requeriría hacerlo es mucho mayor al necesesario para comer el hash. De allí se dice que proviene el nombre.\n",
        "\n",
        "Para la mayoría de las aplicaciones, la reversibilidad o la biyección de la función de hash no es necesaria. En la mayoría de las aplicaciones, solo necesitamos que la probabilidad de encontrar 2 hashes iguales sea muy baja. Por ejemplo, hablamos de probabilidades menores a  $1:2^{32}\\sim1:10^9$\n",
        "\n",
        "### Tablas de hashes\n",
        "\n",
        "En líneas generales, para insertar un nuevo valor en nuestra tabla de hashes lo que hacemos es lo siguiente:\n",
        "```\n",
        "def replace(table, key, value)\n",
        "    hash = hash_function(key)\n",
        "    table[hash] = (key, value)\n",
        "```\n",
        "En este caso, hemos elegido reemplazar el viejo par de `(key, value)` por uno nuevo. Sin embargo, podríamos elegir revisar si el viejo `key` ya estaba y luego insertarlo en caso de que no estuviera. También podríamos elegir almacenar una lista de keys con igual hash, despues de todo la probabilidad de contrar dos iguales es pequeña, pero no cero. El problema de la colisión (¿qué hacer cuando 2 keys coinciden?) es algo que dependerá de la aplicación, así como de las necesidades y decisiones de diseño. Por suerte, `python` ya ha tomado estas decisiones por usted y también provee soluciones alternativas como en el modulo `collections`\n",
        "\n",
        "De alguna manera, está discusión sobre diccionarios nos da una alternativa al problema de la traducción. Podríamos tener una especie de diccionario con todas las palabras del inglés como keys y todas las palabras del español como values. De esa manera, aprenderíamos que palabras se traducen como cuales. El problema de esto es múltiple:\n",
        "\n",
        "* Las lenguas humanas no funciónan así. Fijese en el ejemplo anterior como el caracter を no tiene un correspondiente en el español\n",
        "* ¿Que pasa con palabras homofonas y de diferente valor semántico? ¿Como definiríamos a cual hacerle caso?\n",
        "  * Frente al banco, había una plaza con bancos verdes\n",
        "* ¿Que pasa con palabras nuevas que no están en nuestro diccionario?\n",
        "* ¿Podríamos **entrenar** una estructura de estas características?\n",
        "\n",
        "Esta última pregunta es la que está en el centro de la discusión. El diccionario que planteamos es en esencia una entidad discreta. Si la palabra no está en nuestro diccionario (si no hay colisión) no obtenemos traducción. Mientras que nuestras redes son continuas, si encuentran algo levemente distinto a algo que ya vieron lo reconoceran. Además, es esa continuidad la que permite el entrenamiento, pues nos permite buscar mínimos para nuestra función de pérdidad. Lo que ahora intentaremos será construir una función que haga las veces de un diccionario continuo."
      ],
      "metadata": {
        "id": "Gxp7Rrg44KeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atención\n"
      ],
      "metadata": {
        "id": "K7fZmrBQLlWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discutamos por un segundo como es el acceso a un diccionario. Usaremos también python como guía para un pseudocódigo:\n",
        "\n",
        "```\n",
        "def access(table, query):\n",
        "    hash = hash_function(query) # usamos un hash\n",
        "    if table[hash].key == query: # vemos si encontramos algo\n",
        "        return table[hash].value # lo devolvemos.\n",
        "    return None # si no hay nada devolvemos None.\n",
        "```\n",
        "\n",
        "En realidad, la primera parte del diccionario ya la tenemos resulta. En efecto, nuestras redes recurrentes convierten cada token en un vector de dimensión arbitraria. Convierten un string en una serie de números. No es un índice, como lo hace una función de hash. Pero una red recurrente es función que suponemos lo suficientmente potente como para guardar todo lo que la red ha visto hasta ese momento.\n",
        "\n",
        "Ahora, lo que necesitamos resuelver es el problema de las últimas 3 líneas. En las últimas 3 líneas decimos que devolveremos algo **solámente** si la coincidencia es total. Es aquí donde tenemos algo que podemos suavizar y la respuesta nos la da las compuertas de LSTM y GRU. Habíamos dicho que en una memoria real, usabamos 1 y 0 para activar o desactivar el guardado de información. Nuestra solución en ese caso fue suavizar (smoothing) logrando una salida entre 0 y 1, en lugar de solo 0 o 1. Aquí haremos algo parecido.\n",
        "\n",
        "En esencia, lo que hacemos en el código anterior es devolver `table[hash].value` si la similitud entre nuestra consulta `query` y alguna clave de nuestro diccionario `table[hash].key` es total. Como ahora queremos generar un diccionario continuo y entrenable, debemos definir una función de similitud. Esta función de similitud actuará comparando `query` y `table[hash].key` y deberá dar 1 si la coincidencia es total o 0 si no hay coincidencia. Además podrá devolver cualquie valor intermedio entre 1 y 0\n",
        "\n",
        "Al trabajar con similitudes, podemos ahora cuantificar la similitud entre cada una de las claves `kys` dentro de `table` y el valor `query` que nosotros usamos para consultar el diccionario. Los valores de similitud nos dicen que tan parecida debe ser la salida a cada `value` almacenado. Si la similitud entre `key1` y `query` es 1, la salida sera `value1`. Si la similitud es 0, `value1` no contribuira a la salida. Sea $f(\\mathbf{q}_i,\\mathbf{k}_j)$ una función similitud entre la $i$-ésima consulta $\\mathbf{q}_i$ y la $j$-ésima clave $\\mathbf{k}_j$, entonces la salida de nuestro diccionario está dada por:\n",
        "\n",
        "$$\\mathbf{o}_j = \\sum_{i=0}^{N}f(\\mathbf{q}_j,\\mathbf{k}_i)\\mathbf{v}_i$$\n",
        "\n",
        "Donde $\\mathbf{v}_i$ es el $i$-ésimo valor asociado a la $i$-ésima clave $\\mathbf{k}_i$\n",
        "\n",
        "En resumen, un **mecanismo de atención** compara consultas con claves preexistentes y devuelve una combinación lineal de valores, donde los pesos son la similitud entre cada clave y la consulta. La principal diferencia entre los mecanismo de atención en abstracto y el mecanismo de **atención de Bahdanau** es que en el caso del mecanismo de Bahdanau, las claves $\\mathbf{k}_i$ y los valores $\\mathbf{v}_i$ coinciden."
      ],
      "metadata": {
        "id": "rQj2nD_AXC_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/qcxZdzX.gif)\n"
      ],
      "metadata": {
        "id": "3DMLfSdZ_I9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de atención"
      ],
      "metadata": {
        "id": "cKRMXfVsEZlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mencionaremos aquí 3 de las funciones de similitud más usadas en atención de Bahdanau"
      ],
      "metadata": {
        "id": "qc3PfxlCEiq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atención aditiva\n",
        "\n",
        "Está es probablemente la más evidente de las 3. Dado que queremos generar una función entrenable en el contexto de redes neuronales, ¿Porque no usar MLP?\n",
        "\n",
        "En efecto, se define un MLP de 2 capas, como una función de similitud.\n",
        "\n",
        "$$\\mathbf{h}_{ij} = \\tanh(\\mathbf{W}_{hq}\\mathbf{q}_j+\\mathbf{W}_{hk}\\mathbf{k}_i)$$\n",
        "\n",
        "$$s_{ij} = \\text{softmax}(\\mathbf{w}_{h}\\mathbf{h}_{ij})$$\n",
        "\n",
        "$$\\mathbf{o}_j = \\sum_{i=0}^{N}s_{ij}\\mathbf{v}_i$$\n"
      ],
      "metadata": {
        "id": "QhHJzPJVEuO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atención con producto escalar.\n",
        "\n",
        "Está es la más sencilla de las 3. Por lo general, cuando uno piensa en medidas de simiitud, el producto escalar es la primera en ser mencionada. Por esto definimos la similitud de la siguiente manera:\n",
        "\n",
        "$$s_{ij} = \\text{softmax}\\left(\\dfrac{\\mathbf{q}^\\top_j\\mathbf{k}_i}{\\sqrt{d}}\\right)$$\n",
        "\n",
        "$$\\mathbf{o}_j = \\sum_{i=0}^{N}s_{ij}\\mathbf{v}_j$$\n",
        "\n",
        "En este caso, necesitamos que $\\mathbf{q}_j$ y $\\mathbf{k}_i$ tengan la misma dimensión. Además, hemos supuesto que ambos vectores siguen una distribución normal, por eso que que los normalizamos usando $d$, el número de componentes del vector. Por último, para segurarnos de que se comporten como una similitud, aplicamos $\\text{softmax}$ a lo largo de cada clave $\\mathbf{k}_i$. Con estos valores generamos la combinación lineal de valores."
      ],
      "metadata": {
        "id": "TGnuhRJHJTyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Producto escalar generalizado.\n",
        "\n",
        "La idea es simplmente una continuación de lo anterior. En lugar de usar el producto escalar, usaremos una generalización del producto escalar. Para ello usaremos una matriz entre $\\mathbf{q}_j$ y $\\mathbf{k}_i$\n",
        "\n",
        "$$s_{ij} = \\text{softmax}\\left(\\dfrac{\\mathbf{q}^\\top_j\\mathbf{W}_{qk}\\mathbf{k}_i}{\\sqrt{d}}\\right)$$\n",
        "\n",
        "$$\\mathbf{o}_j = \\sum_{i=0}^{N}s_{ij}\\mathbf{v}_j$$\n",
        "\n",
        "Un detalle de esta implementación es que para usar correctamente a los pesos $\\mathbf{W}_{qk}$ debemos crearlos con el constructor `nn.Bilinear` de `torch`, seteando el atributo `bias` en `False`\n",
        "\n"
      ],
      "metadata": {
        "id": "1rsywckdONZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usando atención."
      ],
      "metadata": {
        "id": "CYGAfF0TYCcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de los discutido en la clase de Seq2Seq, veremos que modificacioens hemos hecho al código. Primero importaremos las mismas interfaces que habíamos usado en la clase anterior"
      ],
      "metadata": {
        "id": "rWXVcw9B_pQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Más tarde puede haber argumentos adicionales\n",
        "    # (por ejemplo, longitud para excluir el relleno)\n",
        "    def forward(self, X, *args):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "IMiAMZKaS5ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Más tarde puede haber argumentos adicionales\n",
        "    # (por ejemplo, longitud para excluir el relleno)\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "AjWpa1OkR9N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X, *args):\n",
        "        enc_outputs = self.encoder(enc_X, *args)\n",
        "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
        "        return self.decoder(dec_X, dec_state)[0]"
      ],
      "metadata": {
        "id": "PzedBSz3XVjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego analicemos los cambios que hemos hecho al encoder. La principal diferencia en este caso es que usaremos una sola capa de GRU, pero la haremos bidimensional"
      ],
      "metadata": {
        "id": "NGVS94VhBHZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_RNN(module):\n",
        "    if type(module) == nn.Linear:\n",
        "         nn.init.xavier_uniform_(module.weight)\n",
        "    if type(module) == nn.GRU:\n",
        "        for param in module._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(module._parameters[param])\n",
        "\n",
        "class ATTNEncoder(Encoder):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.apply(init_RNN)\n",
        "    \"\"\"\n",
        "    Diferencias con el código de la clase anterior:\n",
        "        * Trabajamos con 1 sola capa\n",
        "        * RNN bidireccional ¿Que ventaja trae?\n",
        "        * Aplicamos un capa densa adicional para convertir la\n",
        "          salida bidireccional en una unidireccional\n",
        "        * Hay variables que no estamos guardando como la dimensión\n",
        "          de los estados ocultos.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        \"\"\"\n",
        "        Necesitamos adapatar la salida bidireccional del encoder\n",
        "        a la salida unidireccional del decore (¿Por qué?)\n",
        "        \"\"\"\n",
        "\n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "\n",
        "        #hidden --> [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs son siempre de la última capa\n",
        "\n",
        "        #hidden [-2, :, : ] último forwards RNN\n",
        "        #hidden [-1, :, : ] último backwards RNN\n",
        "\n",
        "        \"\"\"\n",
        "        Concatenamos el último estado y lo metemos en una capa densa\n",
        "        \"\"\"\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "cq26dXXihAJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora en la última dimensión de `outputs`, tenemos el doble de dimensiones que en `num_hidden`. Justamente, tenemos una"
      ],
      "metadata": {
        "id": "pNGKPsIVOj5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
        "batch_size, num_steps = 4, 9\n",
        "\n",
        "encoder = ATTNEncoder(vocab_size, embed_size, num_hiddens, num_hiddens, 0.5)\n",
        "X = torch.zeros((num_steps, batch_size), dtype=torch.int32)\n",
        "outputs, state = encoder(X)\n",
        "print(outputs.shape, (num_steps,batch_size,num_hiddens * 2))\n",
        "print(state.shape, (batch_size, num_hiddens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHGX_MC5jXC0",
        "outputId": "2944e4e9-b874-45d2-bbee-1161cf5e9951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9, 4, 32]) (9, 4, 32)\n",
            "torch.Size([4, 16]) (4, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora debemos adapatar las salidas del encoder para que puedan ser procesadas por el mecanismo de atención y por el decoder."
      ],
      "metadata": {
        "id": "wCqX4J6SDuem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ATTNDecoder(Decoder):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = attention #atención!\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_dim = output_dim\n",
        "        self.apply(init_RNN)\n",
        "    \"\"\"\n",
        "    Diferencias con el código de la clase anterior:\n",
        "        * Trabajamos con 1 sola capa\n",
        "        * hay variables que no estamos guardando como la dimensión\n",
        "          de los estados ocultos.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Como el estado inicial hidden lo generamos con una capa densa,\n",
        "    ahora no podemos recuperarlo sin llamar al encoder.\n",
        "        El método init_state queda sin implementar!\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "\n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        \"\"\"\n",
        "        Ahora aplicamos dropout\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        \"\"\"\n",
        "        Llamamos a la función de atención para generar los pesos\n",
        "        que usaremos para calcular el estado oculto que necesita\n",
        "        el decoder\n",
        "        \"\"\"\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "        #a = [batch size, src len]\n",
        "\n",
        "        a = a.unsqueeze(1) # pesos\n",
        "        #a = [batch size, 1, src len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "        \"\"\"\n",
        "        Aplicamos multplicación de matrices por minilotes:\n",
        "            batch matrix multiplication = bmm\n",
        "        Esta función recibe dos minilotes de matrices y luego los\n",
        "        multiplica elemento a elemento\n",
        "            result = []\n",
        "            for A, B in zip (batch_A, batch_B):\n",
        "                C = A @ B\n",
        "                result.append(C)\n",
        "        \"\"\"\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "        \"\"\"\n",
        "        Concatenamos para luego pasar el valor a RNN\n",
        "        \"\"\"\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "\n",
        "        \"\"\"\n",
        "        En este caso seq len, n layers, n directions valen 1\n",
        "        output = [1, batch size, dec hid dim]\n",
        "        hidden = [1, batch size, dec hid dim]\n",
        "        \"\"\"\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "\n",
        "        \"\"\"\n",
        "        Capa final.\n",
        "        \"\"\"\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "metadata": {
        "id": "7MiiT6JPfU2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por último analicemos nuestro mecanismo de atención."
      ],
      "metadata": {
        "id": "JEo1fn_9FSaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "            Recordemos que tenemos 2 MLP:\n",
        "                * Uno para el output del encoder\n",
        "                * otro para generar el estado oculto.\n",
        "        \"\"\"\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        \"\"\"\n",
        "        Repetimos el estado oculto del decoder src_len veces\n",
        "        \"\"\"\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        # hidden = [batch size, 1, dec hid dim]\n",
        "\n",
        "        # hidden = [batch size, src len, dec hid dim]\n",
        "        #                        ^ todos estos índices son iguales\n",
        "\n",
        "        \"\"\"\n",
        "        Cambiamos el orden de los índice.\n",
        "        \"\"\"\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "        \"\"\"\n",
        "        Aplicamos primera capa de MLP\n",
        "        \"\"\"\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
        "\n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        \"\"\"\n",
        "        Aplicamos segunda capa de MLP\n",
        "        \"\"\"\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        #attention= [batch size, src len]\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "JmKZKmnFdRBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No hay grandes diferencias en nuestro Seq2Seq con teacher forcing. Solo hemos evitado usar el método `init_state`. La razón es que ahora esta cantidad es manejada por nuestro decoder por medio del mecanismo de atención."
      ],
      "metadata": {
        "id": "m7lLtZ14FXMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class ATTNSeq2Seq(EncoderDecoder):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        \"\"\"\n",
        "        Como ahora usamos anteción, el estado inicial es manejado\n",
        "        por el decoder.\n",
        "        \"\"\"\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "MVoz70K8rY9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargando los Datos\n",
        "\n"
      ],
      "metadata": {
        "id": "vAGmz5MuAvRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import re\n",
        "from shutil import unpack_archive\n",
        "\n",
        "data = None\n",
        "#!wget -O spa-eng.zip http://www.manythings.org/anki/spa-eng.zip\n",
        "if not os.path.isfile(\"spa.txt\"):\n",
        "    unpack_archive('./spa-eng.zip', extract_dir='./', format='zip')\n",
        "with open('./spa.txt', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "    data = re.sub(\"\\tCC-BY 2\\.0.*\",\"\",data) # acá elimino información adicional\n",
        "    data = re.sub(r\"[\\u202f]|[\\xa0]\",\" \",data) # aca saco caracteres raros\n",
        "    data = re.sub(\"([,\\.:;!?])\",\" \\\\1\",data) # aca  y abajo tokenizo puntuación\n",
        "    data = re.sub(\"([¡¿])\",\"\\\\1 \",data).lower()\n"
      ],
      "metadata": {
        "id": "S_WUXllMA-3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IopirAQHVvQt",
        "outputId": "0dd42900-6675-47c2-fee1-1715b9dd02bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go .\tve .\n",
            "go .\tvete .\n",
            "go .\tvaya .\n",
            "go .\tváyase .\n",
            "hi .\thola .\n",
            "run !\t¡ corre !\n",
            "run !\t¡ corran !\n",
            "run !\t¡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "SRC_IDX, TGT_IDX = 0, 1\n",
        "\n",
        "SEED = 12312\n",
        "\n",
        "data2 = data.split('\\n')\n",
        "random.seed(SEED)\n",
        "random.shuffle(data2)\n",
        "\n",
        "data_list = []\n",
        "for i, line in enumerate(data2):\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "        # Skip empty tokens\n",
        "        new_src = [t for t in f'{parts[SRC_IDX]} <eos>'.split(' ') if t]\n",
        "        new_tgt = [t for t in f'<bos> {parts[TGT_IDX]} <eos>'.split(' ') if t]\n",
        "        length_src = len(new_src)\n",
        "        data_list.append((new_src, length_src, new_tgt))\n",
        "\n",
        "print(data_list[0][0], data_list[0][-1])\n",
        "print(len(data_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VeFH-9-BuSX",
        "outputId": "89c7dee9-778a-42de-a929-317589ad5db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tom', 'seldom', 'puts', 'sugar', 'in', 'his', 'coffee', '.', '<eos>'] ['<bos>', 'tom', 'casi', 'nunca', 'le', 'pone', 'azúcar', 'al', 'café', '.', '<eos>']\n",
            "138440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n = len(data_list)\n",
        "split1, split2 = int(0.7*n), int(0.9*n)\n",
        "train_list = data_list[:split1]\n",
        "val_list = data_list[split1:split2]\n",
        "test_list = data_list[split2:]\n",
        "\n",
        "counter_src, counter_tgt = Counter(), Counter()\n",
        "for i in range(len(train_list)):\n",
        "  counter_src.update(train_list[i][0])\n",
        "  counter_tgt.update(train_list[i][-1])\n",
        "\n",
        "vocab_src = vocab(counter_src, min_freq = 2,\n",
        "              specials=('<unk>', '<eos>', '<bos>', '<pad>'))\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "\n",
        "vocab_tgt = vocab(counter_tgt, min_freq = 2,\n",
        "              specials=('<unk>', '<eos>', '<bos>', '<pad>'))\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])\n"
      ],
      "metadata": {
        "id": "MlU1LfacCAhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "class BucketSampler(Sampler):\n",
        "\n",
        "    def __init__(self, batch_size, train_list):\n",
        "        self.length = len(train_list)\n",
        "        self.train_list = train_list\n",
        "        self.batch_size = batch_size\n",
        "        indices = [(i, s[1]) for i, s in enumerate(self.train_list)]\n",
        "        random.seed(SEED)\n",
        "        random.shuffle(indices)\n",
        "        pooled_indices = []\n",
        "        # creamos minilotes de tamaños similares\n",
        "        for i in range(0, len(indices), batch_size * 100):\n",
        "            pooled_indices.extend(sorted(indices[i:i + batch_size * 100],\n",
        "                                         key=lambda x: x[1], reverse=True))\n",
        "\n",
        "        self.pooled_indices = pooled_indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, len(self.pooled_indices), self.batch_size):\n",
        "            yield [idx for idx, _ in self.pooled_indices[i:i + self.batch_size]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.length + self.batch_size - 1) // self.batch_size"
      ],
      "metadata": {
        "id": "YjpbkiK2CMV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "SRC_PAD_IDX = vocab_src['<pad>']\n",
        "TGT_PAD_IDX = vocab_tgt['<pad>']\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_src, length_list, text_tgt_in, text_tgt_out = [], [], [], []\n",
        "    for (src, length, tgt) in batch:\n",
        "        # convertimos el texto en tokens\n",
        "        processed_src = torch.tensor([vocab_src[token] for token in src])\n",
        "        processed_tgt = torch.tensor([vocab_tgt[token] for token in tgt])\n",
        "        text_src.append(processed_src)\n",
        "        text_tgt_in.append(processed_tgt[:-1])\n",
        "        text_tgt_out.append(processed_tgt[1:])\n",
        "        # guardamos la longitud de cada token\n",
        "        length_list.append(length)\n",
        "    # armamos la tupla que conformara un ejemplo de minilote.\n",
        "    result = (pad_sequence(text_src, padding_value=SRC_PAD_IDX),\n",
        "              pad_sequence(text_tgt_in, padding_value=TGT_PAD_IDX),\n",
        "              pad_sequence(text_tgt_out, padding_value=TGT_PAD_IDX),)\n",
        "    return result"
      ],
      "metadata": {
        "id": "ipx7mj_YCOrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # A batch size of 64\n",
        "\n",
        "train_bucket = BucketSampler(batch_size, train_list)\n",
        "train_iter = DataLoader(train_list,\n",
        "                          batch_sampler=train_bucket,\n",
        "                          collate_fn=collate_batch)\n",
        "\n",
        "val_bucket = BucketSampler(batch_size, val_list)\n",
        "val_iter = DataLoader(val_list,\n",
        "                          batch_sampler=val_bucket,\n",
        "                          collate_fn=collate_batch)\n",
        "\n",
        "test_bucket = BucketSampler(batch_size, test_list)\n",
        "test_iter = DataLoader(test_list,\n",
        "                          batch_sampler=test_bucket,\n",
        "                          collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "2T5ppiECCSCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src, input, out = next(iter(test_iter))\n",
        "\n",
        "a = [vocab_src.get_itos()[token] for example in src.T[:1] for token in example]\n",
        "b = [vocab_tgt.get_itos()[token] for example in out.T[:1] for token in example]\n",
        "c = [vocab_tgt.get_itos()[token] for example in input.T[:1] for token in example]\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "kpKpN6OJCUEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e27bc8-dc95-4c2b-c727-9a4a3bc53b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in', '1969', ',', 'roger', 'miller', '<unk>', 'a', 'song', 'called', '\"you', \"don't\", 'want', 'my', 'love', '.\"', 'today', ',', 'this', 'song', 'is', 'better', 'known', 'as', '<unk>', 'the', 'summer', 'time', '.\"', \"it's\", 'the', 'first', 'song', 'he', 'wrote', 'and', 'sang', 'that', 'became', 'popular', '.', '<eos>']\n",
            "['en', '1969', ',', 'roger', 'miller', 'grabó', 'una', 'canción', 'llamada', '<unk>', 'no', 'quieres', 'mi', '<unk>', '.', 'hoy', ',', 'esta', 'canción', 'es', 'más', 'conocida', 'como', '\"en', 'el', '<unk>', '.', 'es', 'la', 'primera', 'canción', 'que', 'escribió', 'y', 'cantó', 'que', 'se', 'convirtió', 'popular', '.', '<eos>']\n",
            "['<bos>', 'en', '1969', ',', 'roger', 'miller', 'grabó', 'una', 'canción', 'llamada', '<unk>', 'no', 'quieres', 'mi', '<unk>', '.', 'hoy', ',', 'esta', 'canción', 'es', 'más', 'conocida', 'como', '\"en', 'el', '<unk>', '.', 'es', 'la', 'primera', 'canción', 'que', 'escribió', 'y', 'cantó', 'que', 'se', 'convirtió', 'popular', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bucle de Entrenamiento"
      ],
      "metadata": {
        "id": "TX3dksqNhj6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, tgt_input, tgt_out = batch\n",
        "        src, tgt_input, tgt_out = src.to(device), tgt_input.to(device), tgt_out.to(device)\n",
        "        #src: son las frases en el idioma origen que le pasaremos como entrada al encoder\n",
        "        #src.shape : [src len, batch size]\n",
        "        #tgt_input: son las frases en el idioma destino que le pasaremos como entrada al decoder (con `<bos>` como primer token y sin `<eos>`)\n",
        "        #tgt_out: son las frases en el idioma destino que usaremos para calcular la pérdida (con `<eos>` como finalizador de oración y sin `<bos>`)\n",
        "        #tgt.shape : [trg len, batch size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        #output = [trg len, batch size, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        #como la función de pérdida solo funciona en entradas 2d con objetivos 1d,\n",
        "        # necesitamos aplanar cada una de ellas con .view\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = tgt_out[1:].view(-1)\n",
        "        #trg = [trg len * batch size]\n",
        "        #output = [trg len * batch size, output dim]\n",
        "\n",
        "        #calculamos los gradientes\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        #recortamos los gradientes para evitar que exploten\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "I3lCQgDphwHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, tgt_input, tgt_out = batch\n",
        "            src, tgt_input, tgt_out = src.to(device), tgt_input.to(device), tgt_out.to(device)\n",
        "            output = model(src, tgt_input, 0) #turn off teacher forcing\n",
        "            #output = [trg len, batch size, output dim]\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.view(-1, output_dim)\n",
        "            trg = tgt_out.view(-1)\n",
        "            #trg = [trg len * batch size]\n",
        "            #output = [trg len * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "1T33a6A6orp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "IpHOKK_mrT-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "INPUT_DIM = len(vocab_src.get_itos())\n",
        "OUTPUT_DIM = len(vocab_tgt.get_itos())\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = ATTNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = ATTNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = ATTNSeq2Seq(enc, dec, device).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)"
      ],
      "metadata": {
        "id": "-OcpXZ9OrzOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento.\n",
        "\n",
        "Luego, entrenamos nuestro modelo, guardando los parámetros que nos den la mejor pérdida de validación."
      ],
      "metadata": {
        "id": "KCSWyK8uh7hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "id": "3pzWdWzNhobv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "#model.load_state_dict(torch.load('tut2-model.pt', map_location=torch.device('cpu')))\n",
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuUf-GYFBUwi",
        "outputId": "9b349fbc-c03f-4f0e-8bda-0f0767626e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 3.748 | Test PPL:  42.452 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción"
      ],
      "metadata": {
        "id": "Il3Gm2jYDF6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tratemos ahora de generar predicciones para frases nuevas."
      ],
      "metadata": {
        "id": "9W-NQl1YFw0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#src = \"the car is red . <eos>\"\n",
        "#tgt = \"el coche es rojo .\"\n",
        "\n",
        "src = \"a red car and three dogs . <eos>\"\n",
        "tgt = \"un coche rojo y tres perros .\"\n",
        "\n",
        "tkn_src = [ vocab_src[tkn] for tkn in src.split(' ')]\n",
        "input_src = torch.Tensor(tkn_src)\n",
        "input_src = input_src.type(torch.int32)\n",
        "input_src = input_src.unsqueeze(1)\n",
        "input_src = input_src.to(device)\n",
        "\n",
        "model.eval()\n",
        "outputs, hidden = model.encoder(input_src)\n",
        "\n",
        "final = []\n",
        "matrix = []\n",
        "input_tgt = torch.Tensor([vocab_tgt['<bos>']])\n",
        "string = ''\n",
        "for i in range(9):\n",
        "    if string == '<eos>':\n",
        "        break\n",
        "    input_tgt = input_tgt.type(torch.int32)\n",
        "    input_tgt = input_tgt.to(device)\n",
        "    a = model.decoder.attention(hidden, outputs)\n",
        "    next_tkn, hidden = model.decoder(input_tgt, hidden, outputs)\n",
        "    input_tgt = next_tkn.argmax(1)\n",
        "    string = vocab_tgt.get_itos()[int(input_tgt)]\n",
        "    final.append(string)\n",
        "    matrix.append(a)\n",
        "\n",
        "print(src)\n",
        "print(tgt)\n",
        "print(final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWJaRcsD5mY7",
        "outputId": "fd41b935-7018-4a41-f3e1-68c66a0337c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a red car and three dogs . <eos>\n",
            "un coche rojo y tres perros\n",
            "['coche', 'rojo', 'un', 'y', 'tres', 'perros', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de alinemiento."
      ],
      "metadata": {
        "id": "wadocq_VGxx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "#from matplotlib_inline import backend_inline\n",
        "\n",
        "#backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\n",
        "                  cmap='Reds'):\n",
        "    \"\"\"Show heatmaps of matrices.\"\"\"\n",
        "    num_rows, num_cols = len(matrices), len(matrices[0])\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize,\n",
        "                                 sharex=True, sharey=True, squeeze=False)\n",
        "    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n",
        "        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n",
        "            pcm = ax.imshow(matrix.detach(), cmap=cmap)\n",
        "            ax.set_xticks(np.arange(len(xlabel)))\n",
        "            ax.set_xticklabels(xlabel)\n",
        "            ax.set_yticks(np.arange(len(ylabel)))\n",
        "            ax.set_yticklabels(ylabel)\n",
        "            if titles:\n",
        "                ax.set_title(titles[j])\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "    fig.colorbar(pcm, ax=axes, shrink=0.6);\n",
        "\n",
        "\n",
        "heat = torch.cat(matrix).to(torch.device('cpu'))\n",
        "\n",
        "heat = heat.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "show_heatmaps(heat, xlabel=src.split(' '), ylabel=final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "jYGxfWMT5Wbx",
        "outputId": "d14317fe-c580-4731-eca5-7c0c76593f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 180x180 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU"
      ],
      "metadata": {
        "id": "dO9YZAsXc0QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU es un métrica para analizar la calidad de nuestras traducciones.\n",
        "![](https://i.imgur.com/e1Y5gV3.jpg)\n",
        "![](https://i.imgur.com/AOst2T2.jpg)\n",
        "![](https://i.imgur.com/t7lFALV.jpg)\n",
        "![](https://i.imgur.com/WYzVdqB.jpg)"
      ],
      "metadata": {
        "id": "mjhuV2rxwWSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación mostraremos que es lo que debe hacer en pseudo código y luego mostraremos una función para calcularla\n",
        "\n",
        "\n",
        "* $BLEU$($pred$: `List`, $label$: `List`, $k$: `in`):\n",
        "    * `if` $length\\text{\\_}pred< l$: `break`\n",
        "    * $lenght\\text{\\_}pred$ ⟵ $longitud(pred)$\n",
        "    * $lenght\\text{\\_}label$ ⟵ $longitud(label)$\n",
        "    * $score$ ⟵ $\\exp\\left(\\min\\left(0, 1- \\dfrac{lenght\\text{\\_}label}{lenght\\text{\\_}pred}\\right)\\right)$\n",
        "    * `for` $l$ `in` $range(1, k)$:\n",
        "        * $NGramas\\text{_}pred$ ⟵ $NGramas(pred,~ size = l)$\n",
        "        * $NGramas\\text{_}label$ ⟵ $NGramas(label,~ size = l)$\n",
        "        * $N$ ⟵ $coincidencias(NGramas\\text{\\_}pred,~ NGramas\\text{\\_}label)$\n",
        "        * $p$ ⟵ $\\dfrac{N}{length\\text{\\_}pred+1-l}$\n",
        "        * $score$ ⟵ $score \\times p^{1/2^l}$\n",
        "    * `return` $score$\n",
        "\n",
        "La idea es que BLEU penaliza si no hay coincidencias con N-Gramas, y además penaliza si la oración es demasiado larga. Esto último es porque mientras más larga es una oración, más probable es que acierten los N-Gramas."
      ],
      "metadata": {
        "id": "oV-A5wWzdLEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "# convertimos nuestras oraciones en secuencias de palabras\n",
        "pred_tokens, label_tokens = tgt.split(' '), final\n",
        "\n",
        "# eliminamos el <eos> de final\n",
        "label_tokens = label_tokens[:-1]\n",
        "\n",
        "# calculamos la longitud\n",
        "len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
        "\n",
        "# iniciamos el primer valor de puntaje.\n",
        "score = math.exp(min(0, 1 - len_label / len_pred))\n",
        "\n",
        "k = 2 # SOLO USAMOS 2-GRAMAS!\n",
        "# ciclo\n",
        "for n in range(1, min(k, len_pred) + 1):\n",
        "    num_matches, label_subs = 0, collections.defaultdict(int)\n",
        "\n",
        "    for i in range(len_label - n + 1):\n",
        "        ## Generamos todos los n-gramas de tamaño i para labels\n",
        "        label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
        "\n",
        "    for i in range(len_pred - n + 1):\n",
        "        ## Generamos todos los n-gramas de tamaño i para labels\n",
        "        if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
        "            #contamos coincidencias.\n",
        "            num_matches += 1\n",
        "            label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
        "\n",
        "    # actualizamos score\n",
        "    score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
        "\n",
        "print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYOE3PsrT20j",
        "outputId": "c63a690a-e3b8-46a0-990d-f1bd408b77f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8801117367933934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search\n",
        "\n",
        "Hasta aquí, para la predicción hemos usado lo que se llama \"greed search\" elegimos siempre \"el mejor\" ejemplo. Sin embargo , esto no siempre es una buena estrategia. Por esto, es necesario presentar otras formas de generar predicciones.\n",
        "\n",
        "![](https://i.imgur.com/Yrrg0zq.gif)"
      ],
      "metadata": {
        "id": "auoXxGo1sWjt"
      }
    }
  ]
}