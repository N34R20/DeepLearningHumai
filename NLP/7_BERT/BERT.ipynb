{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/7_BERT/BERT.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "6xBj2-DpQOF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "\n",
        "El a√±o 2018 fue  un importante punto de inflexi√≥n para los modelos de aprendizaje autom√°tico que manejan texto (o, m√°s exactamente, Procesamiento del Lenguaje Natural o NLP para abreviar). Nuestra comprensi√≥n de la mejor forma de representar palabras y oraciones comprendiendo los significados y las relaciones subyacentes est√° evolucionando r√°pidamente. Adem√°s, la comunidad de NLP ha estado presentando componentes incre√≠blemente poderosos que puedes descargar y usar libremente en tus propios modelos y versiones\n",
        "\n",
        "![](https://jalammar.github.io/images/transformer-ber-ulmfit-elmo.png)\n",
        "\n",
        "ULM-FiT no tiene nada que ver con Cookie Monster pero no se me ocurri√≥ nada mejor üôÇ\n",
        "\n",
        "Uno de los √∫ltimos hitos en este desarrollo es el lanzamiento de BERT, un evento descrito como el comienzo de una nueva era en la NLP. BERT es un modelo que rompi√≥ varios r√©cords relativos a la forma en la que estos modelos pueden manejar tareas basadas en el lenguaje. Poco despu√©s del lanzamiento del documento que presenta Bert, el equipo liber√≥ el c√≥digo del modelo y puso a libre disposici√≥n la descarga de versiones del modelo pre-entrenadas con conjuntos de datos masivos. Este es un desarrollo trascendental, ya que permite a cualquier persona desarrollar un modelo de aprendizaje autom√°tico que involucre procesamiento del lenguaje, usando este motor como un componente f√°cilmente disponible, ahorrando el tiempo, la energ√≠a, el conocimiento y los recursos que habr√≠a tenido que destinar a entrenar un modelo de procesamiento del lenguaje construido desde cero.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-transfer-learning.png)\n",
        "Estos son los 2 pasos a seguir para usar BERT. Primero descargas el modelo previamente entrenado con datos no anotados (PASO 1) y luego te concentras en ajustarlo (PASO 2). [Fuente de la imagen].\n",
        "\n",
        "BERT se basa en algunas buenas ideas que han ido surgiendo recientemente en la comunidad de NLP, y que incluyen, entre otros, el aprendizaje semi-supervisado (de Andrew Dai y Quoc Le), ELMo (de Matthew Peters e investigadores de AI2 y UW CSE ), ULMFiT (del fundador de fast.ai Jeremy Howard y Sebastian Ruder), transformer OpenAI (de los investigadores de OpenAI Radford, Narasimhan, Salimans y Sutskever) y los Transformer (de Vaswani et alia.) .\n",
        "\n",
        "Conviene conocer bien algunos conceptos esenciales para comprender correctamente qu√© es BERT. Pero comencemos por ver las formas en que puedes usar BERT, antes de ver los conceptos involucrados en el modelo en s√≠.\n",
        "\n",
        "## Clasificaci√≥n de oraciones\n",
        "\n",
        "La forma m√°s directa de emplear BERT es usarlo para clasificar un fragmento de texto. El modelo tendr√≠a este aspecto:\n",
        "\n",
        "![](https://jalammar.github.io/images/BERT-classification-spam.png)\n",
        "\n",
        "Para entrenar un modelo de este tipo, principalmente se tiene que entrenar el clasificador, con cambios m√≠nimos en el modelo BERT durante la fase de entrenamiento. Este proceso de entrenamiento se llama Fine-Tunning y tiene sus ra√≠ces en el Aprendizaje Secuencial Semi-supervisado y ULM-FiT.\n",
        "\n",
        "Para las personas que no est√°n versadas en el tema, dado que hablamos de clasificadores, estamos en el dominio del aprendizaje supervisado, dentro del campo del aprendizaje autom√°tico. Lo que significar√≠a que necesitamos un conjunto de datos etiquetados para entrenar dicho modelo. Para este ejemplo de clasificador de spam, el conjunto de datos etiquetado ser√≠a una lista de mensajes de correo electr√≥nico y una etiqueta (‚Äúspam‚Äù o ‚Äúno spam‚Äù para cada mensaje).\n",
        "\n",
        "![](https://jalammar.github.io/images/spam-labeled-dataset.png)\n",
        "\n",
        "Otros ejemplos de este uso podr√≠an incluir:\n",
        "\n",
        "* An√°lisis de sentimientos\n",
        "  * Entrada: Rese√±a de pel√≠cula/producto. Salida: ¬øla revisi√≥n es positiva o negativa?\n",
        "  * Conjunto de datos de ejemplo: SST\n",
        "* Comprobaci√≥n de hechos\n",
        "  * Entrada: oraci√≥n. Salida: ‚ÄúDeclaraci√≥n‚Äù o ‚ÄúNo declaraci√≥n‚Äù\n",
        "  * Ejemplo m√°s ambicioso/futurista:\n",
        "      * Entrada: Declaraci√≥n. Salida: ‚ÄúVerdadera‚Äù o ‚ÄúFalsa‚Äù\n",
        "\n",
        "## Arquitectura del modelo\n",
        "\n",
        "Ahora que sabemos en que podemos aplica BERT, echemos un vistazo m√°s de cerca a c√≥mo funciona.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-base-bert-large.png)\n",
        "\n",
        "El paper original del BERT presenta dos tama√±os del modelo:\n",
        "\n",
        "* BERT BASE: comparable en tama√±o al Transformer OpenAI (para comparar rendimiento)\n",
        "* BERT LARGE: un modelo rid√≠culamente enorme que logr√≥ los incre√≠bles resultados rese√±ados en el paper original\n",
        "\n",
        "BERT es b√°sicamente una pila de transformers encoders entrenados. Es posible que sea necesario revisar la clase de Transformes para continuar\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-base-bert-large-encoders.png)\n",
        "\n",
        "Ambos tama√±os de modelos BERT tienen una gran cantidad de capas de encoder: 12 para la versi√≥n b√°sica y 24 para la versi√≥n grande. Estos tambi√©n tienen densas m√°s grandes (768 y 1024 unidades ocultas respectivamente) y m√°s cabezales de atenci√≥n (12 y 16 respectivamente) que la configuraci√≥n predeterminada en la implementaci√≥n de referencia del Transformer en el paper inicial (6 capas de codificador, 512 unidades ocultas, y 8 cabezales de atenci√≥n).\n",
        "\n",
        "|Caracter√≠sticas|Transformer|BERT base|BERT large|\n",
        "|:--|:-:|:-:|:-:|\n",
        "|Cantidad de capas|6|12|24|\n",
        "|Longitud de estados ocultos|512|768|1024|\n",
        "|Cantidad de cabezales de atenci√≥n|8|12|16|\n",
        "\n",
        "## Entradas del modelo\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-input-output.png)\n",
        "\n",
        "El primer token de entrada viene con un token [CLS] especial por motivos que se aclarar√°n m√°s adelante. CLS aqu√≠ significa Clasificaci√≥n.\n",
        "\n",
        "Al igual que el encoder vanilla del transformer, BERT toma una secuencia de palabras como entrada que avancia hacia arriba en la pila. Cada capa aplica auto atenci√≥n, pasa sus resultados a trav√©s de una red de avance y luego los devuelve al siguiente encoder.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-encoders-input.png)\n",
        "\n",
        "En t√©rminos de arquitectura, es id√©ntica a la del Transformer hasta este punto (aparte del tama√±o, que no deja de ser una configuraciones a elecci√≥n de cada uno). Es en la salida donde empezamos a ver c√≥mo cambian las cosas.\n",
        "\n",
        "## Salidas del modelo\n",
        "\n",
        "Cada posici√≥n genera un vector de tama√±o hidden_size (768 en BERT Base). Para el ejemplo de clasificaci√≥n de oraciones que vimos anteriormente, nos enfocamos en la salida de solo la primera posici√≥n (a la que le pasamos el token [CLS] especial).\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-output-vector.png)\n",
        "\n",
        "Ese vector ahora se puede usar como entrada para un clasificador de nuestra elecci√≥n. El documento logra excelentes resultados usando √∫nicamente una red neuronal de una sola capa como clasificador.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-classifier.png)\n",
        "\n",
        "Si tienes m√°s etiquetas (por ejemplo, si es un servicio de correo electr√≥nico que etiqueta los correos electr√≥nicos como ‚Äúspam‚Äù, ‚Äúno spam‚Äù, ‚Äúsocial‚Äù y ‚Äúpromoci√≥n‚Äù), simplemente modifica la red clasificadora para tener m√°s neuronas de salida que luego pasar por softmax.\n",
        "\n",
        "## Una nueva era de integraci√≥n\n",
        "\n",
        "Estos nuevos desarrollos traen consigo un nuevo cambio en la forma en que se codifican las palabras. Hasta ahora, los embeddings de palabras han sido una gran herramienta en la forma en que los principales modelos de NLP tratan el lenguaje. M√©todos como Word2Vec y Glove se han utilizado ampliamente para este tipo de tareas. Recapitulemos c√≥mo se usan antes de se√±alar lo que ahora ha cambiado.\n",
        "\n",
        "Resumen de embeddings de palabras\n",
        "\n",
        "Para que las palabras sean procesadas por modelos de aprendizaje autom√°tico, necesitan alguna forma de representaci√≥n num√©rica que los modelos puedan usar en sus c√°lculos. Word2Vec demostr√≥ que podemos usar un vector (una lista de n√∫meros) para representar correctamente las palabras de una manera que captura las relaciones sem√°nticas o relacionadas con el significado (por ejemplo, la capacidad de saber si las palabras son similares u opuestas, o que un par de palabras como ‚ÄúEstocolmo‚Äù y ‚ÄúSuecia‚Äù tienen entre ellos la misma relaci√≥n que tienen entre ellos ‚ÄúEl Cairo‚Äù y ‚ÄúEgipto‚Äù), as√≠ como relaciones sint√°cticas o basadas en la gram√°tica (por ejemplo, la relaci√≥n entre ‚Äúten√≠a‚Äù y ‚Äútengo‚Äù es la mismo que entre ‚Äúera‚Äù y ‚Äúes‚Äù).\n",
        "\n",
        "La comunidad r√°pidamente se dio cuenta de que era mucho mejor idea usar embeddings previamente entrenados con grandes cantidades de datos de texto, en lugar de entrenarlas junto con el modelo en lo que con frecuencia eran peque√±os conjuntos de datos. De esta  manera uno descarga una lista de palabras y sus embeddings a partir del entrenamiento previo con Word2Vec o GloVe. Este es un ejemplo del embedding GloVe de la palabra ‚Äúpalo‚Äù (con un tama√±o de vector de embedding de 200)\n",
        "\n",
        "![](https://jalammar.github.io/images/glove-embedding.png)\n",
        "\n",
        "Embedding GloVe de la palabra ‚Äúpalo‚Äù: un vector de 200 floats (redondeados a dos decimales). Contin√∫a con m√°s de doscientos valores.\n",
        "\n",
        "Dada su longitud y la gran cantidad de n√∫meros, en las ilustraciones de mis publicaciones utilizo esta cuadr√≠cula simplificada para representar vectores:\n",
        "\n",
        "![](https://jalammar.github.io/images/vector-boxes.png)\n",
        "\n",
        "## ELMo: el contexto importa\n",
        "\n",
        "Si estamos usando esta representaci√≥n GloVe, entonces la palabra ‚Äúpalo‚Äù estar√≠a representada por este vector sin importar el contexto. Sin embargo, es claro que esto es un problema.(Peters et. al., 2017 , McCann et. al., 2017 , y una vez m√°s Peters et. al., 2018 en el art√≠culo de ELMo ). \"Palo\" tiene m√∫ltiples significados dependiendo de d√≥nde se use. ¬øPor qu√© no darle un embedding basado en el contexto en el que se usa, tanto para capturar el significado de la palabra en ese contexto como para otra informaci√≥n contextual?‚Äù. Y as√≠ nacieron los embeddings de palabras contextualizadas .\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-embedding-robin-williams.png)\n",
        "\n",
        "Los embeddings de palabras contextualizadas pueden dar a las palabras embeddings diferentes seg√∫n el significado que tengan en el contexto de la oraci√≥n.\n",
        "\n",
        "En lugar de usar un embedding fijo para cada palabra, ELMo analiza la oraci√≥n completa antes de asignarle un embedding a cada palabra. Utiliza un LSTM bidireccional entrenado en una tarea espec√≠fica para poder crear esos embedding.\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-word-embedding.png)\n",
        "\n",
        "ELMo supuso un paso significativo hacia la formaci√≥n previa en el contexto de la NLP. El ELMo LSTM se entrenar√≠a en un conjunto de datos masivo en el idioma de nuestro conjunto de datos, y luego podemos usarlo como componente en otros modelos que necesitan manejar el idioma.\n",
        "\n",
        "¬øCu√°l es el secreto de ELMo?\n",
        "\n",
        "ELMo adquiri√≥ su comprensi√≥n del idioma al ser entrenado para predecir la siguiente palabra en una secuencia de palabras, una tarea llamada **Modelado del Lenguaje**. Esto es muy interesante porque tenemos grandes cantidades de datos de texto de los que dicho modelo puede aprender sin necesidad de etiquetas.\n",
        "\n",
        "![](https://jalammar.github.io/images/Bert-language-modeling.png)\n",
        "\n",
        "Uno de los pasos en el proceso de pre-entrenamiento de ELMo: dado ‚ÄúLets stick to‚Äù como entrada, predecir la siguiente palabra m√°s probable: una tarea de Modelado de Lenguaje . Cuando se entrena en un gran conjunto de datos, el modelo comienza a captar patrones de lenguaje. Es poco probable que adivine con precisi√≥n la siguiente palabra en este ejemplo. De manera m√°s realista, despu√©s de una palabra como ‚Äúpasar‚Äù, asignar√° una mayor probabilidad a una palabra como ‚Äútiempo‚Äù (para conformar ‚Äúpasar tiempo‚Äù) que a ‚Äúc√°mara‚Äù.\n",
        "\n",
        "Podemos entrever cada uno de los pasos de LSTM asomando por detr√°s de la cabeza de ELMo. Resultan muy √∫tiles para generar embeddings de que realizar este entrenamiento previo.\n",
        "\n",
        "ELMo en realidad va un paso m√°s all√° y entrena un LSTM bidireccional, de modo que su modelo de lenguaje no solo se hace una idea de la palabra siguiente, sino tambi√©n de la palabra anterior.\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png)\n",
        "\n",
        "ELMo llega al  embedding contextualizado mediante la agrupaci√≥n de los estados ocultos (y el embedding inicial) de cierta manera (concatenaci√≥n seguida de suma ponderada).\n",
        "\n",
        "![](https://jalammar.github.io/images/elmo-embedding.png)\n",
        "\n",
        "## ULM-FiT: Transferencia de Aprendizaje en PNLP\n",
        "\n",
        "ULM-FiT introdujo nuevos m√©todos para utilizar de manera m√°s efectiva mucho de lo que el modelo aprende durante el entrenamiento previo, m√°s all√° de los meros embeddings e embeddings contextualizados. ULM-FiT introdujo un nuevo modelo de lenguaje y un proceso para ajustar efectivamente ese modelo de lenguaje para resolver varias tareas.\n",
        "\n",
        "## El Transformer: yendo m√°s all√° que los LSTM\n",
        "\n",
        "La publicaci√≥n del paper y el c√≥digo de Transformer, y los resultados que logr√≥ en tareas como la traducci√≥n autom√°tica, comenzaron a hacer que algunos pensaran en ellos como un reemplazo de los LSTM. Esto se vio agravado por el hecho de que los Transformers manejan las dependencias a largo plazo mejor que los LSTM.\n",
        "\n",
        "La estructura Encoder-Decoder del Transformer lo hizo perfecto para la traducci√≥n autom√°tica. Pero, ¬øc√≥mo lo usar√≠as para la clasificaci√≥n de oraciones? ¬øC√≥mo lo usar√≠a para entrenar previamente un modelo de idioma que se puede ajustar para otras tareas (tareas posteriores es lo que el campo llama tareas de aprendizaje supervisado que utilizan un modelo o componente entrenado previamente).\n",
        "\n",
        "## OpenAI Transformer: pre-entrenando un decoder de Transformer para Modelado de Lenguaje\n",
        "\n",
        "Resulta que no necesitamos un Transformer completo para adoptar transferencia del aprendizaje y un modelo de lenguaje ajustable para tareas de NLP. Podemos hacerlo solo con el decoder del Transformer. El decoder es una buena opci√≥n porque es una opci√≥n natural para el **modelado del lenguaje** (predecir la siguiente palabra) ya que est√° dise√±ado para **enmascarar tokens futuros**, una caracter√≠stica valiosa cuando genera una traducci√≥n palabra por palabra.\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-transformer-1.png)\n",
        "\n",
        "El transformer OpenAI est√° integrado por la pila de decoders del Transformer\n",
        "\n",
        "El modelo apil√≥ doce capas de decoder. Dado que no hay encoder en esta configuraci√≥n, estas capas de decoder no tendr√≠an la subcapa de atenci√≥n de encoder-decoder que tienen las capas de decoder de Transformer est√°ndar. Sin embargo, todav√≠a tendr√≠a la capa de auto atenci√≥n (enmascarada para que no alcance su punto m√°ximo en tokens futuros).\n",
        "\n",
        "Con esta estructura, podemos proceder a entrenar el modelo en la misma tarea de modelado de lenguaje: predecir la siguiente palabra utilizando conjuntos de datos masivos **sin etiquetar**. ¬°M√©tele 7.000 libros y que aprenda! Los libros son excelentes para este tipo de tareas, ya que permiten que el modelo aprenda a asociar informaci√≥n relacionada, incluso si est√°n separados por mucho texto, algo que no se obtiene, por ejemplo, cuando se entrena con tweets o art√≠culos.\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-transformer-language-modeling.png)\n",
        "\n",
        "OpenAI Transformer listo para ser entrenado para predecir la siguiente palabra, con un conjunto de datos compuesto por 7.000 libros.\n",
        "\n",
        "Transferencia del Aprendizaje a tareas posteriores\n",
        "\n",
        "Ahora que el Transformer OpenAI est√° preentrenado y sus capas se han ajustado para manejar razonablemente el lenguaje, podemos comenzar a usarlo para tareas posteriores. Primero veamos la clasificaci√≥n de oraciones (clasificar un mensaje de correo electr√≥nico como ‚Äúspam‚Äù o ‚Äúno spam‚Äù):\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-transformer-sentence-classification.png)\n",
        "\n",
        "C√≥mo usar un Transformer OpenAI preentrenado para clasificar oraciones\n",
        "\n",
        "El documento de OpenAI describe una serie de transformaciones de entrada para manejar las entradas para diferentes tipos de tareas. La siguiente imagen del paper muestra las estructuras de los modelos y las transformaciones de entrada para llevar a cabo diferentes tareas.\n",
        "\n",
        "![](https://jalammar.github.io/images/openai-input%20transformations.png)\n",
        "\n",
        "Muy inteligente, no?\n",
        "\n",
        "### BERT: de Decodificadores a Codificadores\n",
        "\n",
        "El Transformer openAI nos brind√≥ un modelo preentrenado ajustable con precisi√≥n basado en el Transformer. Pero faltaba algo en esta transici√≥n de LSTM a Transformers. El modelo de lenguaje de ELMo era bidireccional, pero el Transformer openAI solo entrena un modelo de lenguaje directo. ¬øPodr√≠amos construir un modelo basado en Transformers cuyo modelo de lenguaje mire tanto hacia adelante como hacia atr√°s (en la jerga t√©cnica, ‚Äúest√© condicionado tanto en el contexto izquierdo como en el derecho‚Äù)?\n",
        "\n",
        "Modelo de lenguaje enmascarado\n",
        "\n",
        "Todo el mundo sabe que el condicionamiento bidireccional permitir√≠a que cada palabra se viera indirectamente en un contexto de varias capas. Por eso no se usa encoders para esta tarea... ¬øPero que pasa si usamos m√°scaras?\n",
        "\n",
        "![](https://jalammar.github.io/images/BERT-language-modeling-masked-lm.png)\n",
        "\n",
        "El proceso de modelado de lenguaje inteligente de BERT enmascara el 15% de las palabras en la entrada y le pide al modelo que prediga la palabra que falta.\n",
        "\n",
        "Es literalmente la tarea de cualquier libro de texto de lengua extranjera. Completar con la preposici√≥n correcta, con la conjugaci√≥n correcta. Solo que ahora es como si le dijeramos al alumno. Podes usar todo el diccionario.\n",
        "\n",
        "Encontrar la tarea correcta para entrenar una pila de codificadores de Transformer es un obst√°culo complejo que BERT resuelve adoptando un concepto de ‚Äúmodelo de lenguaje enmascarado‚Äù de la literatura anterior (donde se denomina tarea Cloze).\n",
        "\n",
        "M√°s all√° de enmascarar el 15% de la entrada, BERT tambi√©n mezcla un poco las cosas para mejorar la forma en que el modelo se ajusta m√°s tarde. A veces **reemplaza aleatoriamente una palabra con otra palabra** y le pide al modelo que prediga la palabra correcta en esa posici√≥n.\n",
        "\n",
        "## Tareas de dos oraciones\n",
        "\n",
        "Si miramos atr√°s en las transformaciones de entrada que hace el Transformer OpenAI para gestionar diferentes tareas, notaremos que algunas tareas requieren que el modelo diga algo inteligente sobre dos oraciones (por ejemplo, ¬øson simplemente versiones parafraseadas una de la otra? Dada una entrada de wikipedia como entrada, y una pregunta sobre esa entrada como otra entrada, ¬øpodemos responder esa pregunta?).\n",
        "\n",
        "Para hacer que BERT sea mejor en el manejo de relaciones entre m√∫ltiples oraciones, el proceso de entrenamiento previo incluye una tarea adicional: dadas dos oraciones (A y B), ¬øes probable que B sea la oraci√≥n que sigue a A, o no?\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-next-sentence-prediction.png)\n",
        "\n",
        "La segunda tarea en la que BERT est√° pre-entrenado es una tarea de clasificaci√≥n de dos oraciones. La tokenizaci√≥n esta muy simplificada en este gr√°fico, ya que BERT en realidad usa WordPieces como tokens en lugar de palabras, por lo que algunas palabras se dividen en fragmentos m√°s peque√±os.\n",
        "\n",
        "## Modelos espec√≠ficos de tareas\n",
        "\n",
        "El paper de BERT muestra varias formas de usar BERT para diferentes tareas.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-tasks.png)\n",
        "\n",
        "BERT para extracci√≥n de caracter√≠sticas\n",
        "\n",
        "El enfoque de Fine-Tuning no es la √∫nica forma de utilizar BERT. Al igual que ELMo, se puede usar BERT previamente entrenado para crear embeddings de palabras contextualizadas. Luego, se puede alimentar estos embeddings a un modelo existente, un proceso que el paper muestra que produce resultados no muy lejanos del Fine-Tuning de BERT en tareas como el reconocimiento de entidades.\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-contexualized-embeddings.png)\n",
        "\n",
        "¬øQu√© vector funciona mejor como embeddig contextualizada?Depende de la tarea. El documento examina seis opciones (en comparaci√≥n con el modelo ajustado que logr√≥ una puntuaci√≥n de 96,4):\n",
        "\n",
        "![](https://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "PXuRSh6PLnAs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhAqZLs3lwhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14698aa7-c073-4de1-dd81-27afd9872027"
      },
      "source": [
        "# Fist install the library and download the models from github\n",
        "\n",
        "!pip install transformers\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
        "!tar -xzvf pytorch_weights.tar.gz\n",
        "!mv config.json pytorch/.\n",
        "!mv vocab.txt pytorch/."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.7 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
            "--2022-09-05 17:31:41--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 409871727 (391M) [application/x-gzip]\n",
            "Saving to: ‚Äòpytorch_weights.tar.gz‚Äô\n",
            "\n",
            "pytorch_weights.tar 100%[===================>] 390.88M  9.05MB/s    in 81s     \n",
            "\n",
            "2022-09-05 17:33:04 (4.83 MB/s) - ‚Äòpytorch_weights.tar.gz‚Äô saved [409871727/409871727]\n",
            "\n",
            "--2022-09-05 17:33:04--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 242120 (236K) [text/plain]\n",
            "Saving to: ‚Äòvocab.txt‚Äô\n",
            "\n",
            "vocab.txt           100%[===================>] 236.45K   253KB/s    in 0.9s    \n",
            "\n",
            "2022-09-05 17:33:07 (253 KB/s) - ‚Äòvocab.txt‚Äô saved [242120/242120]\n",
            "\n",
            "--2022-09-05 17:33:07--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 313 [application/json]\n",
            "Saving to: ‚Äòconfig.json‚Äô\n",
            "\n",
            "config.json         100%[===================>]     313  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-05 17:33:08 (41.7 MB/s) - ‚Äòconfig.json‚Äô saved [313/313]\n",
            "\n",
            "pytorch/\n",
            "pytorch/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_1ucCCUnXqa"
      },
      "source": [
        "# import the necessary\n",
        "\n",
        "import torch\n",
        "from transformers import BertForMaskedLM, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCpOxoXkoGFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8fbee8-dd57-41d8-f6ff-f9b1e432acb3"
      },
      "source": [
        "# create the tokenizer and the model\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
        "model = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
        "e = model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KXo6-ahoJoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ab651c-af3a-40ea-fa66-b3742b84d4c8"
      },
      "source": [
        "# Now test it\n",
        "\n",
        "# Miguel de Cervantes, El ingenioso hidalgo Don Quijote de la Mancha\n",
        "\n",
        "text = \"[CLS] En un lugar [MASK] la [MASK] de cuyo nombre no quiero [MASK] [SEP]\"\n",
        "masked_indxs = (4,6,12)\n",
        "\n",
        "\n",
        "# Jos√© Hernandez, Martin Fierro\n",
        "\"\"\"\n",
        "text = \"[CLS] Los hermanos sean [MASK] [SEP] porque esa es la ley primera; [SEP] \\\n",
        "tengan uni√≥n verdadera [SEP] en cualquier tiempo que [MASK], [SEP] \\\n",
        "porque si entre [MASK] pelean [SEP] los devoran los de afuera. [SEP]\"\n",
        "masked_indxs = (4,22,28)\n",
        "\"\"\"\n",
        "# Alejandro Dolina, Lo que me cost√≥ el amor de Laura.\n",
        "#text = \"[CLS] se ha dicho que los [MASK], hacen todo lo que [MASK] con el √∫nico fin de [MASK] mujeres\"\n",
        "#masked_indxs = (6,12,18)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "predictions = model(tokens_tensor)[0]\n",
        "\n",
        "for i,midx in enumerate(masked_indxs):\n",
        "    idxs = torch.argsort(predictions[0,midx], descending=True)\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
        "    print('MASK',i,':',predicted_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MASK 0 : ['de', 'en', 'como', 'a', ',']\n",
            "MASK 1 : ['tierra', 'ciudad', 'Tierra', 'gente', 'patria']\n",
            "MASK 2 : ['hablar', '.', 'ser', ',', 'decir']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForNextSentencePrediction, BertTokenizer\n",
        "\n",
        "model2 = BertForNextSentencePrediction.from_pretrained(\"pytorch/\")\n",
        "\n",
        "text = \"[CLS] M√≠rela bien.  [SEP] Ya no la ver√° nunca m√°s. \"\n",
        "\n",
        "text = \"[CLS] Si el espacio es infinito estamos en cualquier punto del espacio. [SEP] Si el tiempo es infinito estamos en cualquier punto del tiempo.\"\n",
        "\n",
        "text = \"[CLS] Si el espacio es infinito estamos en cualquier punto del espacio.  [SEP] Ya no la ver√° nunca m√°s. \"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "predictions = model2(tokens_tensor)\n",
        "\n",
        "print(torch.argmax(predictions.logits))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X1xdDDov-jk",
        "outputId": "b486a63c-6e85-4154-f8df-9200948233b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at pytorch/ were not used when initializing BertForNextSentencePrediction: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_ner = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"mrm8488/RuPERTa-base-finetuned-pos\",\n",
        "    tokenizer=\"mrm8488/RuPERTa-base-finetuned-pos\")\n",
        "\n",
        "text = 'tras el temporal, el joven encontro un empleo temporal'\n",
        "text = 'Hacia el este fue donde este sujeto se dirigio y se topo con este'\n",
        "\n",
        "\n",
        "outputs = nlp_ner(text)\n",
        "\n",
        "for i in outputs:\n",
        "  print(i['word'], i['entity'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1XwLc2o63zq",
        "outputId": "756aa647-81d9-4f8d-b972-36c004002c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hacia VERB\n",
            "ƒ†el DET\n",
            "ƒ†este NOUN\n",
            "ƒ†fue AUX\n",
            "ƒ†donde PRON\n",
            "ƒ†este DET\n",
            "ƒ†sujeto NOUN\n",
            "ƒ†se PRON\n",
            "ƒ†diri VERB\n",
            "gio VERB\n",
            "ƒ†y CCONJ\n",
            "ƒ†se PRON\n",
            "ƒ†topo VERB\n",
            "ƒ†con ADP\n",
            "ƒ†este PRON\n"
          ]
        }
      ]
    }
  ]
}