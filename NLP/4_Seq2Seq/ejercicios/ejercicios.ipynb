{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ejercicios",
      "provenance": [],
      "collapsed_sections": [
        "8Ke8ufJfxrUz",
        "vAGmz5MuAvRA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wZ1eXVmN8NTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/4_Seq2Seq/ejercicios/ejercicios.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "OOeOzi93GK1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio Clase 4"
      ],
      "metadata": {
        "id": "xYoP47ig-o01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este notebook modificaremos el encoder del modelo de traducción automática visto en la clase para utilizar una RNN bidireccional. Al igual que en el modelo anterior, usamos un GRU de dos capas, sin embargo ahora la haremos bidireccional. Con un RNN bidireccional, tenemos dos RNN en cada capa. Un *RNN hacia adelante* que repasa los embedding de la oración de izquierda a derecha (que se muestra a continuación en verde), y un *RNN hacia atrás* que repasa los embedding de la oración de derecha a izquierda (verde azulado). Todo lo que necesitamos hacer en el código es establecer `bidirectional = True` y luego pasar los embedding de la oración al RNN como antes.\n",
        "\n",
        "![](https://i.imgur.com/bdUnvj9.png)\n",
        "\n",
        "Ahora tenemos:\n",
        "\n",
        "$$\\begin{align*}\n",
        "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n",
        "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n",
        "\\end{align*}$$\n",
        "\n",
        "Donde $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n",
        "\n",
        "Como antes, solo pasamos una entrada(`embedded`) al RNN, que le dice a PyTorch que inicialice los estados ocultos iniciales hacia adelante y hacia atrás ($ h_0 ^ \\rightarrow $ y $ h_0 ^ \\leftarrow $, respectivamente) como un tensor de todos los ceros. También obtendremos dos vectores de contexto, uno del RNN hacia adelante después de haber visto la última palabra en la oración, $ z ^ \\rightarrow = h_T ^ \\rightarrow $, y uno del RNN hacia atrás después de haber visto la primera palabra en la oración, $ z ^ \\leftarrow = h_T ^ \\leftarrow $.\n",
        "\n",
        "El RNN devuelve `outputs` y `hidden`.\n",
        "\n",
        "`outputs` es de tamaño `[longitud de la oración de origen, tamaño de lote, (dimensión de variables ocultas)x(numero de direcciones)]` donde los primeros ` num_hiddens` elementos en el tercer eje son los estados ocultos de la capa superior hacia adelante RNN, y los últimos `num_hiddens ` elementos son estados ocultos de la capa superior hacia atrás RNN. Podemos pensar en el tercer eje como los estados ocultos hacia adelante y hacia atrás concatenados entre sí, es decir, $ h_1 = [h_1 ^ \\rightarrow; h_{T} ^ \\leftarrow] $, $ h_2 = [h_2 ^ \\rightarrow; h_{T-1} ^ \\leftarrow] $ y podemos denotar todos los estados ocultos del encoder (hacia adelante y hacia atrás concatenados juntos) como $ H = \\{h_1, h_2, ..., h_T \\} $.\n",
        "\n",
        "`hidden` es de tamaño **[número de capas * número de direcciones, tamaño de lote, dimensión de variables ocultas]**, donde `hidden[- 2,:,:]` entregaría el estado oculto de la capa superior de la RNN hacia adelante después del paso de tiempo final (es decir, después de haber visto la última palabra en la oración), `hidden[- 1,:,: ]` entregaría el estado oculto de la capa superior de la RNN hacia atrás después del paso de tiempo final (es decir, después de haber visto la primera palabra en la oración) y así sucesivamente.\n",
        "\n",
        "Como el decoder no es bidireccional, solo necesitaremos un vector de contexto $z$, y actualmente tenemos dos versiones, uno hacia adelante y uno hacia atrás ($ z ^ \\rightarrow = h_T ^ \\rightarrow $ y $ z ^ \\leftarrow = h_T ^ \\leftarrow $, respectivamente). Resolvemos esto concatenando los dos vectores de contexto juntos, pasándolos a través de una capa lineal, $ g $, y aplicando la función de activación $ \\tanh $.\n",
        "\n",
        "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n",
        "\n",
        "Lo mismo sucede con los estados ocultos que pasamos para usar como estado inicial $s_0$ en el decoder. Aquí también definiremos una capa lineal para transformarlos, pero teniendo en cuenta que tendremos que hacerlo capa por capa.\n"
      ],
      "metadata": {
        "id": "BErsixMqjGVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1\n",
        "\n",
        "Teniendo en cuenta las cuestiones de implementación mencionadas en la introducción, modificar el Encoder y el Decoder planteados en la clase 4 para que se procese a la oración de origen con una RNN bidireccional."
      ],
      "metadata": {
        "id": "iB6u2aGeCwOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inserte su código aquí\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Más tarde puede haber argumentos adicionales\n",
        "    # (por ejemplo, longitud para excluir el relleno)\n",
        "    def forward(self, X, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # Más tarde puede haber argumentos adicionales\n",
        "    # (por ejemplo, longitud para excluir el relleno)\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X, *args):\n",
        "        enc_outputs = self.encoder(enc_X, *args)\n",
        "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
        "        # Sólo devuelve la salida del decoder\n",
        "        return self.decoder(dec_X, dec_state)[0]"
      ],
      "metadata": {
        "id": "Xz6fOFVxDofS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_RNN(module):\n",
        "    if type(module) == nn.Linear:\n",
        "         nn.init.xavier_uniform_(module.weight)\n",
        "    if type(module) == nn.GRU:\n",
        "        for param in module._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(module._parameters[param])\n",
        "\n",
        "class RNNEncoder(Encoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, dec_num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(num_hiddens * 2, dec_num_hiddens)\n",
        "        self.hid_dim = num_hiddens\n",
        "        self.n_layers = num_layers\n",
        "        self.apply(init_RNN)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # X shape: (num_steps, batch_size)\n",
        "        embs = self.embedding(X.type(torch.int64))\n",
        "        # embs shape: (num_steps, batch_size, embed_size)\n",
        "        outputs, hidden = self.rnn(embs)\n",
        "\n",
        "        #outputs = [src len, batch size, num_hiddens * num directions]\n",
        "        #hidden = [num_layers * num directions, batch size, num_hiddens]\n",
        "\n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "\n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN\n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "\n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards\n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        outputs = torch.tanh(self.fc(outputs[-1]))\n",
        "\n",
        "        hid_layers = []\n",
        "        for l in range(self.n_layers):\n",
        "            hid_forward = hidden[2*l,:,:]\n",
        "            #hid_forward = [batch size, num_hiddens]\n",
        "            hid_backward = hidden[2*l+1,:,:]\n",
        "            #hid_backward = [batch size, num_hiddens]\n",
        "            hid_layer = torch.tanh(self.fc(torch.cat((hid_forward, hid_backward), dim = 1)))\n",
        "            #hid_layer = [batch size, dec_num_hiddens]\n",
        "            hid_layers.append(hid_layer.unsqueeze(0))\n",
        "\n",
        "        hidden = torch.cat(hid_layers, dim = 0)\n",
        "        #hidden = [num_layers, batch size, dec_num_hiddens]\n",
        "\n",
        "        # recuerde que output devolverá siempre el valor de las variables ocultas (de todas las direcciones) de la última capa\n",
        "        # por cada token en la secuencia de cada elemento del lote (num_steps, batch size, hid dim * n directions) .\n",
        "\n",
        "        \"\"\" rnn_outputs shape: (num_steps, batch size, hid dim * 2) \"\"\"\n",
        "\n",
        "        # recuerde que state devolverá siempre el valor de las variables ocultas de todas las capas\n",
        "        # (y en cada dirección si hay mas de una) de cada elemento del lote. (n layers * n directions, batch_size, num_hiddens)\n",
        "\n",
        "        \"\"\" state shape: (num_layers * 2, batch_size, num_hiddens) \"\"\"\n",
        "\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "HLtobGqNhHzP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNDecoder(Decoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens,\n",
        "                           num_layers, dropout=dropout)\n",
        "        self.linear = nn.Linear(num_hiddens, vocab_size)\n",
        "        self.hid_dim = num_hiddens\n",
        "        self.n_layers = num_layers\n",
        "        self.output_dim = vocab_size\n",
        "        self.apply(init_RNN)\n",
        "\n",
        "    def init_state(self, enc_output, *args):\n",
        "        #se queda con el output del último paso de tiempo\n",
        "        return enc_output\n",
        "\n",
        "    def forward(self, X, context, hidden):\n",
        "        # X shape: (batch_size)\n",
        "        # context shape: (batch_size, num_hiddens)\n",
        "        # hidden shape: (n layers, batch_size, num_hiddens)\n",
        "\n",
        "        input = X.unsqueeze(0)\n",
        "        context = context.unsqueeze(0)\n",
        "\n",
        "        #input shape: (1,batch_size)\n",
        "        #context shape: (1, batch_size, num_hiddens)\n",
        "\n",
        "        embs = self.embedding(input)\n",
        "        # embs shape: (1, batch_size, embed_size)\n",
        "        #print(embs.shape, \"\\n\", context.shape)\n",
        "\n",
        "        # Concatena el token de entrada con el contexto\n",
        "        embs_and_context = torch.cat((embs, context), -1)\n",
        "        #embs_and_context shape: (1, batch size, embed_size + num_hiddens)\n",
        "\n",
        "        #Genera las salidas de la RNN\n",
        "        rnn_outputs, state = self.rnn(embs_and_context, hidden)\n",
        "        # recuerde que output devolverá siempre el valor de las variables ocultas (de todas las direcciones) de la última capa\n",
        "        # por cada token en la secuencia de cada elemento del lote (seq_len, batch size, hid dim * n directions) .\n",
        "        # rnn_outputs shape: (1, batch size, hid dim)\n",
        "        # recuerde que state devolverá siempre el valor de las variables ocultas de todas las capas\n",
        "        # (y en cada dirección si hay mas de una) de cada elemento del lote. (n layers * n directions, batch_size, num_hiddens)\n",
        "        # state shape: (num_layers, batch_size, num_hiddens)\n",
        "\n",
        "        #Genera las probabilidades de cada token en el vocabulario\n",
        "        outputs = self.linear(rnn_outputs.squeeze(0))\n",
        "        #outputs = torch.cat((embs.squeeze(0), rnn_outputs.squeeze(0), context),dim = 1)\n",
        "\n",
        "        # outputs shape: (batch_size, vocab_size)\n",
        "        return outputs, state"
      ],
      "metadata": {
        "id": "tChtRcYChHwv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 2\n",
        "\n",
        "Cree un modelo Seq2Seq que utilice el encoder y el decoder del ejercicio anterior. Entrénelo sobre el dataset inglés-español utilizado en clase"
      ],
      "metadata": {
        "id": "PxAzReBPDuto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### MODELO SEQ2SEQ #######################################\n",
        "import random\n",
        "class Seq2Seq_TF(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"¡Las dimensiones ocultas del encoder y del decoder deben ser iguales!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"¡El número de capas del encoder y del decoder deben ser iguales!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        enc_outputs, enc_hidden = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        context = self.decoder.init_state(enc_outputs)\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden and context\n",
        "            dec_output, dec_hidden = self.decoder(input, context, enc_hidden)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = dec_output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = dec_output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "B74cQadVkwsF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### CARGA DE DATOS #################################\n",
        "import os.path\n",
        "import re\n",
        "from shutil import unpack_archive\n",
        "import random\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torchtext.vocab import vocab\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "data = None\n",
        "!wget -O spa-eng.zip http://www.manythings.org/anki/spa-eng.zip\n",
        "if not os.path.isfile(\"spa.txt\"):\n",
        "    unpack_archive('./spa-eng.zip', extract_dir='./', format='zip')\n",
        "with open('./spa.txt', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "    data = re.sub(\"\\tCC-BY 2\\.0.*\",\"\",data) # acá elimino información adicional\n",
        "    data = re.sub(r\"[\\u202f]|[\\xa0]\",\" \",data) # aca saco caracteres raros\n",
        "    data = re.sub(\"([,\\.:;!?])\",\" \\1\",data) # aca  y abajo tokenizo puntuación\n",
        "    data = re.sub(\"([¡¿])\",\"\\1 \",data).lower()\n",
        "\n",
        "\n",
        "SRC_IDX, TGT_IDX = 0, 1\n",
        "SEED = 12312\n",
        "\n",
        "data2 = data.split('\\n')\n",
        "random.seed(SEED)\n",
        "random.shuffle(data2)\n",
        "\n",
        "data_list = []\n",
        "for i, line in enumerate(data2):\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "        # Skip empty tokens\n",
        "        new_src = [t for t in f'{parts[SRC_IDX]} '.split(' ') if t]\n",
        "        new_tgt = [t for t in f' {parts[TGT_IDX]} '.split(' ') if t]\n",
        "        length_src = len(new_src)\n",
        "        data_list.append((new_src, length_src, new_tgt))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(data_list[0][0], data_list[0][-1])\n",
        "print(len(data_list))"
      ],
      "metadata": {
        "id": "wPpXQJJfl_m9",
        "outputId": "ec2d6ba6-b1b9-412f-99ae-a15aa333046d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-12 22:47:01--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5394805 (5.1M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.14M  2.97MB/s    in 1.7s    \n",
            "\n",
            "2023-10-12 22:47:04 (2.97 MB/s) - ‘spa-eng.zip’ saved [5394805/5394805]\n",
            "\n",
            "['they', 'want', 'me', '\\x01'] ['ellos', 'me', 'quieren', 'a', 'mí', '\\x01']\n",
            "140868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n = len(data_list)\n",
        "split1, split2 = int(0.7*n), int(0.9*n)\n",
        "train_list = data_list[:split1]\n",
        "val_list = data_list[split1:split2]\n",
        "test_list = data_list[split2:]\n",
        "\n",
        "counter_src, counter_tgt = Counter(), Counter()\n",
        "for i in range(len(train_list)):\n",
        "  counter_src.update(train_list[i][0])\n",
        "  counter_tgt.update(train_list[i][-1])\n",
        "\n",
        "vocab_src = vocab(counter_src, min_freq = 2,\n",
        "              specials=('<unk>', '<eos>', '<bos>', '<pad>'))\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "\n",
        "vocab_tgt = vocab(counter_tgt, min_freq = 2,\n",
        "              specials=('<unk>', '<eos>', '<bos>', '<pad>'))\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])\n",
        "\n",
        "class BucketSampler(Sampler):\n",
        "\n",
        "    def __init__(self, batch_size, train_list):\n",
        "        self.length = len(train_list)\n",
        "        self.train_list = train_list\n",
        "        self.batch_size = batch_size\n",
        "        indices = [(i, s[1]) for i, s in enumerate(self.train_list)]\n",
        "        random.seed(SEED)\n",
        "        random.shuffle(indices)\n",
        "        pooled_indices = []\n",
        "        # creamos minilotes de tamaños similares\n",
        "        for i in range(0, len(indices), batch_size * 100):\n",
        "            pooled_indices.extend(sorted(indices[i:i + batch_size * 100],\n",
        "                                         key=lambda x: x[1], reverse=True))\n",
        "\n",
        "        self.pooled_indices = pooled_indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, len(self.pooled_indices), self.batch_size):\n",
        "            yield [idx for idx, _ in self.pooled_indices[i:i + self.batch_size]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.length + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "SRC_PAD_IDX = vocab_src['']\n",
        "TGT_PAD_IDX = vocab_tgt['']\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_src, length_list, text_tgt_in, text_tgt_out = [], [], [], []\n",
        "    for (src, length, tgt) in batch:\n",
        "        # convertimos el texto en tokens\n",
        "        processed_src = torch.tensor([vocab_src[token] for token in src])\n",
        "        processed_tgt = torch.tensor([vocab_tgt[token] for token in tgt])\n",
        "        text_src.append(processed_src)\n",
        "        text_tgt_in.append(processed_tgt[:-1])\n",
        "        text_tgt_out.append(processed_tgt[1:])\n",
        "        # guardamos la longitud de cada token\n",
        "        length_list.append(length)\n",
        "    # armamos la tupla que conformara un ejemplo de minilote.\n",
        "    result = (pad_sequence(text_src, padding_value=SRC_PAD_IDX),\n",
        "              pad_sequence(text_tgt_in, padding_value=TGT_PAD_IDX),\n",
        "              pad_sequence(text_tgt_out, padding_value=TGT_PAD_IDX),)\n",
        "    return result\n",
        "\n",
        "batch_size = 64  # A batch size of 64\n",
        "\n",
        "train_bucket = BucketSampler(batch_size, train_list)\n",
        "train_iter = DataLoader(train_list,\n",
        "                          batch_sampler=train_bucket,\n",
        "                          collate_fn=collate_batch)\n",
        "\n",
        "val_bucket = BucketSampler(batch_size, val_list)\n",
        "val_iter = DataLoader(val_list,\n",
        "                          batch_sampler=val_bucket,\n",
        "                          collate_fn=collate_batch)\n",
        "\n",
        "test_bucket = BucketSampler(batch_size, test_list)\n",
        "test_iter = DataLoader(test_list,\n",
        "                          batch_sampler=test_bucket,\n",
        "                          collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "O5225BdXkwlQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### BUCLE DE ENTRENAMIENTO #################################\n",
        "import time\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, tgt_input, tgt_out = batch\n",
        "        src, tgt_input, tgt_out = src.to(device), tgt_input.to(device), tgt_out.to(device)\n",
        "        #src: son las frases en el idioma origen que le pasaremos como entrada al encoder\n",
        "        #src.shape : [src len, batch size]\n",
        "        #tgt_input: son las frases en el idioma destino que le pasaremos como entrada al decoder (con `` como primer token y sin ``)\n",
        "        #tgt_out: son las frases en el idioma destino que usaremos para calcular la pérdida (con `` como finalizador de oración y sin ``)\n",
        "        #tgt.shape : [trg len, batch size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "        #output = [trg len, batch size, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        #como la función de pérdida solo funciona en entradas 2d con objetivos 1d,\n",
        "        # necesitamos aplanar cada una de ellas con .view\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = tgt_out[1:].view(-1)\n",
        "        #trg = [trg len * batch size]\n",
        "        #output = [trg len * batch size, output dim]\n",
        "\n",
        "        #calculamos los gradientes\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        #recortamos los gradientes para evitar que exploten\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, tgt_input, tgt_out = batch\n",
        "            src, tgt_input, tgt_out = src.to(device), tgt_input.to(device), tgt_out.to(device)\n",
        "            output = model(src, tgt_input, 0) #turn off teacher forcing\n",
        "            #output = [trg len, batch size, output dim]\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.view(-1, output_dim)\n",
        "            trg = tgt_out.view(-1)\n",
        "            #trg = [trg len * batch size]\n",
        "            #output = [trg len * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "metadata": {
        "id": "gkaJyiHglKc8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab_src.get_itos())\n",
        "OUTPUT_DIM = len(vocab_tgt.get_itos())\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq_TF(enc, dec, device).to(device)\n",
        "\n",
        "model = Seq2Seq_TF(enc, dec, device).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TGT_PAD_IDX)\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "id": "lKAbYtdSlNx-",
        "outputId": "294464f1-30e4-4543-c726-5442e97b6788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 55s\n",
            "\tTrain Loss: 6.121 | Train PPL: 455.547\n",
            "\t Val. Loss: 7.022 |  Val. PPL: 1121.546\n",
            "Epoch: 02 | Time: 1m 38s\n",
            "\tTrain Loss: 5.622 | Train PPL: 276.532\n",
            "\t Val. Loss: 6.037 |  Val. PPL: 418.748\n",
            "Epoch: 03 | Time: 1m 46s\n",
            "\tTrain Loss: 4.700 | Train PPL: 109.963\n",
            "\t Val. Loss: 5.444 |  Val. PPL: 231.307\n",
            "Epoch: 04 | Time: 1m 49s\n",
            "\tTrain Loss: 4.058 | Train PPL:  57.855\n",
            "\t Val. Loss: 5.205 |  Val. PPL: 182.153\n",
            "Epoch: 05 | Time: 1m 49s\n",
            "\tTrain Loss: 3.653 | Train PPL:  38.576\n",
            "\t Val. Loss: 5.093 |  Val. PPL: 162.885\n",
            "Epoch: 06 | Time: 1m 39s\n",
            "\tTrain Loss: 3.377 | Train PPL:  29.286\n",
            "\t Val. Loss: 5.038 |  Val. PPL: 154.165\n",
            "Epoch: 07 | Time: 1m 49s\n",
            "\tTrain Loss: 3.193 | Train PPL:  24.361\n",
            "\t Val. Loss: 5.013 |  Val. PPL: 150.417\n",
            "Epoch: 08 | Time: 1m 49s\n",
            "\tTrain Loss: 3.062 | Train PPL:  21.374\n",
            "\t Val. Loss: 5.011 |  Val. PPL: 150.085\n",
            "Epoch: 09 | Time: 1m 49s\n",
            "\tTrain Loss: 2.975 | Train PPL:  19.594\n",
            "\t Val. Loss: 5.010 |  Val. PPL: 149.949\n",
            "Epoch: 10 | Time: 1m 49s\n",
            "\tTrain Loss: 2.916 | Train PPL:  18.473\n",
            "\t Val. Loss: 5.014 |  Val. PPL: 150.433\n"
          ]
        }
      ]
    }
  ]
}